import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import preprocessing
import  joblib
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    roc_auc_score, log_loss, f1_score,
    accuracy_score, precision_score, recall_score,
    balanced_accuracy_score, matthews_corrcoef,
    average_precision_score, brier_score_loss,
    classification_report
)
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay


#ƒê·ªçc d·ªØ li·ªáu
df = pd.read_csv("mental_health_screen_time_dataset.csv")


df.info()
df_numeric = df.select_dtypes(include=['number'])
df_clean =df_numeric.drop(columns=["Participant_ID"])

#ki·ªÉm tra d·ªØ li·ªáu thi·∫øu
miss_value = df_clean.isnull().sum()
miss_precent =(miss_value / len(df_clean))*100
miss_df = pd.DataFrame({
    'Miss Values': miss_value,
    'Miss precent' : miss_precent
})
print(miss_df)


#ki·ªÉm tra d·ªØ li·ªáu thi·∫øu
miss_value = df_clean.isnull().sum()
miss_precent =(miss_value / len(df_clean))*100
miss_df = pd.DataFrame({
    'Miss Values': miss_value,
    'Miss precent' : miss_precent
})
print(miss_df)


# ƒëi·ªÅn c√°c gi√° tr·ªã khuy·∫øt
for column in df_clean.columns:
    # Ki·ªÉm tra xem c·ªôt c√≥ ph·∫£i l√† ki·ªÉu d·ªØ li·ªáu s·ªë hay kh√¥ng (float, int)
    if np.issubdtype(df_clean[column].dtype, np.number):
        # T√≠nh gi√° tr·ªã trung b√¨nh c·ªßa c·ªôt (b·ªè qua c√°c gi√° tr·ªã null)
        mean_value = df_clean[column].mean()
        # Thay th·∫ø c√°c gi√° tr·ªã null b·∫±ng gi√° tr·ªã trung b√¨nh v·ª´a t√≠nh
        df_clean[column].fillna(mean_value, inplace=True)
        print(f"-> ƒê√£ x·ª≠ l√Ω c·ªôt s·ªë '{column}': Thay th·∫ø NaN b·∫±ng gi√° tr·ªã trung b√¨nh ({mean_value:.2f}).")

    elif df_clean[column].dtype == 'object' or df_clean[column].dtype == 'category':

        pass

df_clean.info()


# Ki·ªÉm tra d·ªØ li·ªáu tr√πng l·∫∑p
duplicates = df_clean.duplicated().sum()
duplicates_rows = df[df_clean.duplicated(keep=False)]

print("S·ªë d√≤ng tr√πng l·∫∑p: ", duplicates)
print("\nD√≤ng tr√πng l·∫∑p:\n ", duplicates_rows)


#X√≥a d·ªØ li·ªáu tr√πng
df_clean = df_clean.drop_duplicates()
print(df_clean.shape)


#X√≥a d·ªØ li·ªáu tr√πng
df_clean = df_clean.drop_duplicates()
print(df_clean.shape)


#ƒê·ªïi gi·ªù sang ph√∫t
df_clean['Sleep_Duration'] = df_clean['Sleep_Duration']*60
print(df_clean['Sleep_Duration'].head(10))

df_processed = df_clean.copy()
#ƒê·ªãnh nghƒ©a c√°c nh√≥m c·ªôt theo c√°ch x·ª≠ l√Ω
capping_cols = ['Daily_Screen_Time', 'Phone_Unlocks']
drop_cols = ['App_Work_Time', 'App_Entertainment_Time', 'App_Social_Media_Time']
#H√†m x·ª≠ l√Ω Outlier
def handle_outliers(df, columns, method='capping'):
    for col in columns:
        # T√≠nh to√°n IQR cho t·ª´ng c·ªôt c·ª• th·ªÉ
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        if method == 'capping':
            df[col] = df[col].clip(lower, upper)
        elif method == 'drop':
            # L·ªçc d·ªØ li·ªáu l·∫•y c√°c d√≤ng n·∫±m trong kho·∫£ng an to√†n
            df = df[(df[col] >= lower) & (df[col] <= upper)]
    return df
#Capping cho nh√≥m Daily_Screen_Time v√† Phone_Unlocks
df_processed = handle_outliers(df_processed, capping_cols, method='capping')
#X√≥a d√≤ng cho nh√≥m c√°c ·ª©ng d·ª•ng
df_processed = handle_outliers(df_processed, drop_cols, method='drop')
#Ki·ªÉm tra k·∫øt qu·∫£
print(f"K√≠ch th∆∞·ªõc ban ƒë·∫ßu: {df_clean.shape}")
print(f"K√≠ch th∆∞·ªõc sau khi x·ª≠ l√Ω Outliers: {df_processed.shape}")
# C·∫≠p nh·∫≠t l·∫°i df_clean
df_clean = df_processed.copy()
df = df_clean.copy()

# Feature Engineering
# Log-transform c√°c bi·∫øn h√†nh vi
log_cols = [
    'Daily_Screen_Time',
    'App_Social_Media_Time',
    'App_Work_Time',
    'App_Entertainment_Time',
    'Phone_Unlocks'
]

for col in log_cols:
    df[col + '_log'] = np.log1p(df[col])

# T·∫°o c√°c bi·∫øn t·ª∑ l·ªá
df['Social_Ratio'] = df['App_Social_Media_Time'] / df['Daily_Screen_Time']
df['Entertainment_Ratio'] = df['App_Entertainment_Time'] / df['Daily_Screen_Time']
df['Work_Ratio'] = df['App_Work_Time'] / df['Daily_Screen_Time']

#X·ª≠ l√Ω chia cho 0 v√† gi√° tr·ªã thi·∫øu
df.replace([np.inf, -np.inf], 0, inplace=True)
df.fillna(0, inplace=True)

#T·∫°o c√°c bi·∫øn t∆∞∆°ng t√°c
df['Screen_x_Sleep'] = df['Daily_Screen_Time'] * df['Sleep_Duration']
df['Unlocks_x_Social'] = df['Phone_Unlocks'] * df['App_Social_Media_Time']

#T·∫°o bi·∫øn m·ª•c ti√™u
df['High_Stress'] = (df['Stress_Level'] >= 6).astype(int)

print("\n: Ho√†n th√†nh")


#Feature Selection
feature_cols = [
    'Daily_Screen_Time_log',
    'App_Social_Media_Time_log',
    'App_Work_Time_log',
    'App_Entertainment_Time_log',
    'Phone_Unlocks_log',
    'Social_Ratio',
    'Entertainment_Ratio',
    'Work_Ratio',
    'Screen_x_Sleep',
    'Unlocks_x_Social',
    'Sleep_Duration'
]

X = df[feature_cols]
y = df['High_Stress']

print("S·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng:", X.shape[1])


#Chu·∫©n h√≥a d·ªØ li·ªáu
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

#Chia t·∫≠p
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled,
    y,
    test_size=0.3,
    stratify=y,
    random_state=42
)

print(f"S·ªë m·∫´u hu·∫•n luy·ªán: {X_train.shape[0]}")
print(f"S·ªë m·∫´u ki·ªÉm tra: {X_test.shape[0]}")


#X√¢y d·ª±ng m√¥ h√¨nh
model_config = {
    "penalty": "elasticnet",
    "solver": "saga",
    "l1_ratio": 0.5,
    "class_weight": "balanced",
    "max_iter": 5000,
    "random_state": 42
}

#print("C√°c si√™u tham s·ªë:", model_config)


#Hu·∫•n luy·ªán theo ph∆∞∆°ng ph√°p Hold-out
holdout_model = LogisticRegression(**model_config)
holdout_model.fit(X_train, y_train)


#Hu·∫•n luy·ªán v·ªõi Stratified K-Fold Cross Validation
#Kh·ªüi t·∫°o Stratified K-Fold v·ªõi 5 fold
kfold = StratifiedKFold(
    n_splits=5,        #S·ªë l∆∞·ª£ng fold
    shuffle=True,      #Tr·ªôn d·ªØ li·ªáu tr∆∞·ªõc khi chia
    random_state=42    #C·ªë ƒë·ªãnh seed ƒë·ªÉ ƒë·∫£m b·∫£o t√°i l·∫≠p k·∫øt qu·∫£
)

print("B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán v·ªõi Stratified K-Fold")

kf_results = []

fold = 1  # Bi·∫øn ƒë·∫øm s·ªë th·ª© t·ª± fold

# V√≤ng l·∫∑p qua t·ª´ng fold
for train_idx, val_idx in kfold.split(X_train, y_train):

    print(f"\n--- Fold {fold} ---")

    # X_tr, y_tr: d·ªØ li·ªáu d√πng ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh trong fold hi·ªán t·∫°i
    # X_val, y_val: d·ªØ li·ªáu ki·ªÉm tra n·ªôi b·ªô (validation) trong fold hi·ªán t·∫°i
    X_tr = X_train[train_idx]
    y_tr = y_train.iloc[train_idx]

    X_val = X_train[val_idx]
    y_val = y_train.iloc[val_idx]

    # Kh·ªüi t·∫°o m√¥ h√¨nh Logistic Regression m·ªõi cho m·ªói fold
    # Vi·ªác kh·ªüi t·∫°o l·∫°i ƒë·∫£m b·∫£o c√°c fold l√† ƒë·ªôc l·∫≠p, kh√¥ng b·ªã r√≤ r·ªâ d·ªØ li·ªáu
    model = LogisticRegression(**model_config)

    # Hu·∫•n luy·ªán m√¥ h√¨nh tr√™n t·∫≠p hu·∫•n luy·ªán c·ªßa fold hi·ªán t·∫°i
    model.fit(X_tr, y_tr)

    y_prob = model.predict_proba(X_val)[:, 1]
    y_pred = (y_prob >= 0.5).astype(int)

    kf_results.append({
        "AUC": roc_auc_score(y_val, y_prob),
        "LogLoss": log_loss(y_val, y_prob),
        "F1": f1_score(y_val, y_pred)
    })

    fold += 1

print("Ho√†n t·∫•t qu√° tr√¨nh hu·∫•n luy·ªán v·ªõi Stratified K-Fold")


# 8. L∆∞u model & scaler
# ======================
joblib.dump(model, "logistic_model.pkl")
joblib.dump(scaler, "scaler.pkl")

print("üéâ ƒê√£ l∆∞u logistic_model.pkl v√† scaler.pkl")
